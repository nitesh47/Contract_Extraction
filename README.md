# ğŸ“‘Â ContractÂ Extractor

---

## ğŸš€Â Start

1. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

2. **Set your OpenAI API key in .env**


3. **Run the extractor**
```bash
# Run against every PDF in ./data, write JSONs to ./output
python -m main --directory data --output-dir output --concurrency 2
```

## âœ¨Â Key Features

- ### **Turn PDF into text**
  Using PyPDF2 to extract all the text, page by page.

- ### **Split text into tokens**
  If a contract is too long, we split it into smaller chunks that fit within the modelâ€™s limit. If the tokenizer isnâ€™t there, we just assume every 4Â characters is roughly one token.

- ### **Pick the right schemas**
   Schemas (templates) picked by LLM model in a way that which template fits (like â€œNDAâ€ or â€œService Agreementâ€) to pdf and fills in only valid JSON.

- ### **Fix and combine JSON**
  Sometimes the LLMâ€™s output isnâ€™t perfectly formatted JSON, so we run simple â€œfind and fixâ€ rules to clean it up. Then we merge all the chunks back into one JSON and remove any repeated parties or clauses.

- ### **One JSON file per PDF**
  For Every PDF we will its own `.json` output.

- ### **Pipeline fails**
  If extraction fails or the JSON is unfixable, the pipeline donâ€™t crash: we save a minimal â€œunknownâ€ template and log an error. We also log how long each chunk took and where we saved the file.


## ğŸ—‚ Folder Structure
```
contract_extractor/
â”œâ”€â”€ main.py
â”‚   â””â”€ CLI entry point: orchestrates PDF discovery, LLM client, and concurrency
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ llm_client.py
â”‚       â””â”€ Async wrapper around OpenAIâ€™s chat API
â”‚
â”œâ”€â”€ parsers/
â”‚   â””â”€â”€ pdf_loader.py
â”‚       â””â”€ Extract plain text from PDF pages
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ token_utils.py      â”€ token counting & chunking (real or approximate)
â”‚   â”œâ”€â”€ json_fix.py         â”€ repair nearâ€‘JSON blobs into valid JSON
â”‚   â””â”€â”€ merge_utils.py      â”€ merge chunk outputs + deduplicate parties/clauses
â”‚
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ meta_extraction_prompt.py
â”‚       â””â”€ System prompt template with dynamic schema
â”‚
â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ generic.py          â”€ fallback Pydantic model
â”‚   â”œâ”€â”€ nda.py              â”€ NDAâ€‘specific schema
â”‚   â”œâ”€â”€ employment.py       â”€ Employment contract schema
â”‚   â””â”€â”€ service_agreement.pyâ”€ Service agreement schema
â”‚
â”œâ”€â”€ extractors/
â”‚   â””â”€â”€ metadata_extractor.py
â”‚         â””â”€ Core orchestration: load â†’ chunk â†’ call LLM â†’ merge â†’ save
â”‚
â”œâ”€â”€ data/
â”‚    â””â”€â”€ Add all the pdf files
â”‚
â””â”€â”€ Notebook
   â””â”€â”€ contract_extractor.ipynb
         â””â”€ Jupyter notebook to run the whole code in notebook
```

## â•Â Add a new contract type
#### Create schemas/lease.py:
```
from .generic import GenericContractSchema

class LeaseSchema(GenericContractSchema):
    contract_type = "lease"
    rent_amount: str | None = None
    security_deposit: str | None = None
```

## ğŸ“– Approach & Assumptions
- ### **LLM based**
   Rely on ChatGPT for extraction within the schema constraints.

- ### **Chunking**
   Handles very large contracts by splitting on token limits.

- ### **Deduplication**
   Ensures no repeated parties or clauses in merged output.

- ### **Fallback**
   Falls back on missing dependencies or parsing errors.

- ### **Extensible**
   Schemaâ€‘based design makes it easy to add new contract types.

## Future Improvements
- ### **Evaluation**
   Integrate a labeled dataset (e.g. CUAD) to compute precision/recall.

- ### **Interactive review**
   Build a simple UI for manual correction & approval.

- ### **Hybrid extraction**
   Combine ruleâ€‘based parsers with LLM for higher accuracy.

- ### **Adaptive chunking**
   Dynamically adjust chunk size based on headings or sections.
