{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1844da31",
   "metadata": {},
   "source": [
    "# Contract Metadata Extraction\n",
    "\n",
    "The notebook demonstrates a complete **LLM based** pipeline that:\n",
    "\n",
    "1. Extracts raw text from PDF contracts\n",
    "2. Chunk the pdf if token size is greater than 25000. Since, I am using GPT-4o, it has a token limitation upto 30k.\n",
    "3. Pass the prompt and chunk text to LLM model\n",
    "4. LLM model generates the json output\n",
    "    - If pdf size is less than max token (25000), entire pdf passed to LLM\n",
    "    - If it exceeds 25,000 tokens, the text is divided into multiple chunks. The LLM produces separate JSON for each chunk, which are then de‑duplicated and merged into a single JSON payload.\n",
    "5. OpenAI model **classify** the contract type along with metadata and **return strict JSON** metadata\n",
    "6. Persists the result with guard‑rails for invalid output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38cc219",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f50e0020-79df-4eeb-b976-a095840dfa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "from unidecode import unidecode\n",
    "from itertools import chain\n",
    "import PyPDF2\n",
    "from textwrap import dedent\n",
    "import logging, json, re, time, math, os, hashlib, pathlib\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import AsyncOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, Type, Optional, List, Any, Iterable, Awaitable, Dict, Any\n",
    "from importlib import import_module\n",
    "from pkgutil import iter_modules\n",
    "log = logging.getLogger(__name__)\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "DEFAULT_MODEL = os.getenv(\"DEFAULT_MODEL\")\n",
    "TEMPERATURE = os.getenv(\"TEMPERATURE\")\n",
    "MAX_TOKENS_DEFAULT = int(os.getenv(\"TOKEN_LIMIT\"))\n",
    "TIKTOKEN = os.getenv(\"TIKTOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384b6e2c-189b-4af9-bd0d-35bf8321328b",
   "metadata": {},
   "source": [
    "# Load pdf file from data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec35f1ac-2178-430b-8cf9-c8935a3bf337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_text(path: Path) -> str:\n",
    "    reader = PyPDF2.PdfReader(str(path))\n",
    "    return \"\\n\".join([unidecode(p.extract_text()) or \"\" for p in reader.pages])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beb5474-666c-4324-9601-3a33c6aa774a",
   "metadata": {},
   "source": [
    "# LLM calling function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14936fa9-bffc-4b11-83e0-340c7bdff62e",
   "metadata": {},
   "source": [
    "- ChatMessage (Pydantic model): Typed container for each message in the prompt (role = \"system\", \"user\", or \"assistant\"; content = text).\n",
    "- LLMClient:\n",
    "    - __init__ : Stores the model name and instantiates AsyncOpenAI with OPENAI_API_KEY.\n",
    "    - __aenter__ / __aexit__ : you use async with LLMClient() as client: so the underlying HTTP session is opened/closed cleanly.\n",
    "    - chat_completion : Accepts a list of ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc38393-d55c-498a-9417-79be81a259c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatMessage(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class LLMClient:\n",
    "    def __init__(self, model: str = DEFAULT_MODEL):\n",
    "        self.model = model\n",
    "        self.client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    async def __aenter__(self):\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc, tb):\n",
    "        await self.client.close()\n",
    "\n",
    "    async def chat_completion(self, messages: list[ChatMessage]) -> str:\n",
    "        formatted_messages = [m.model_dump() for m in messages]\n",
    "        response = await self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=formatted_messages,\n",
    "            temperature=float(TEMPERATURE),\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a86f679-fe25-4111-b3e0-8f833c6a09a3",
   "metadata": {},
   "source": [
    "# Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03785f3-d5e1-41fe-aab9-3ece84d31563",
   "metadata": {},
   "source": [
    "### Schema Defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6910610c-6be8-4487-abec-14d5db970b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Party(BaseModel):\n",
    "    role: str\n",
    "    name: str\n",
    "\n",
    "class Clause(BaseModel):\n",
    "    name: str\n",
    "    present: bool\n",
    "    text: Optional[str] = None\n",
    "\n",
    "class GenericContractSchema(BaseModel):\n",
    "    \"\"\"Fallback schema able to hold any contract.\"\"\"\n",
    "    contract_type: str\n",
    "    parties: List[Party] = Field(default_factory=list)\n",
    "    effective_date: Optional[str] = None\n",
    "    termination_date: Optional[str] = None\n",
    "    governing_law: Optional[str] = None\n",
    "    renewal_terms: Optional[str] = None\n",
    "    clauses: List[Clause] = Field(default_factory=list)\n",
    "    custom_fields: Dict[str, Any] = Field(default_factory=dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15cfdf1-cc48-4465-93b6-d2873177bcf3",
   "metadata": {},
   "source": [
    "### Employment Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d330226-bcf2-4c3f-a5ad-dd85f9d38598",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmploymentSchema(GenericContractSchema):\n",
    "    contract_type:str='employment'\n",
    "    employee_name: Optional[str]=None\n",
    "    employer_name: Optional[str]=None\n",
    "    compensation: Optional[str]=None\n",
    "    probation_period: Optional[str]=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1320be6-3e3e-4985-8c35-2049ca53aff4",
   "metadata": {},
   "source": [
    "### NDA Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bedd39e-6b9a-4d59-bfc7-8101a3bc3baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDASchema(GenericContractSchema):\n",
    "    contract_type:str='nda'\n",
    "    confidentiality_period: Optional[str]=None\n",
    "    non_compete: Optional[bool]=None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d152ec15-8155-441f-8993-ca998576b78c",
   "metadata": {},
   "source": [
    "### Service Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0371c63-9d89-47cc-ac07-395a4d49ef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSASchema(GenericContractSchema):\n",
    "    contract_type:str='service agreement'\n",
    "    payment_terms: Optional[str]=None\n",
    "    indemnification: Optional[bool]=None\n",
    "    limitation_of_liability: Optional[str]=None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accc360f-0fe5-4203-af0c-7c183e1fa73c",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "053949f3-71d2-4a55-8c00-090ee018bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def gather_with_concurrency(concurrency:int,*aws:Iterable[Awaitable[Any]])->list[Any]:\n",
    "    sem=asyncio.Semaphore(concurrency)\n",
    "    async def wrap(coro):\n",
    "        async with sem:\n",
    "            return await coro\n",
    "    return await asyncio.gather(*[wrap(a) for a in aws])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c95768-0248-42c9-97c8-a0bb0870c45a",
   "metadata": {},
   "source": [
    "## Pdf Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8346c2a-56de-4773-bb4f-819ddbf37b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_encoder(model: str | None = None):\n",
    "    \"\"\"\n",
    "    Return a tiktoken encoder if available; otherwise fall back to a 4‑chars≈1‑token\n",
    "    approximation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import tiktoken\n",
    "        if model:\n",
    "            return tiktoken.encoding_for_model(model)\n",
    "        return tiktoken.get_encoding(TIKTOKEN)\n",
    "    except Exception:\n",
    "        class _Approx:\n",
    "            def encode(self, text: str) -> list[int]:\n",
    "                return [0] * math.ceil(len(text) / 4)\n",
    "        return _Approx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "721860a5-1c40-468a-937b-30b033f04321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_len(text: str, model: str | None = None) -> int:\n",
    "    try:\n",
    "        return len(_get_encoder(model).encode(text))\n",
    "    except Exception:\n",
    "        return math.ceil(len(text) / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "848232c9-c6b6-4e5e-a61d-4e985f1d93a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_tokens(\n",
    "    text: str,\n",
    "    *,\n",
    "    max_tokens: int = MAX_TOKENS_DEFAULT,\n",
    "    model: str | None = None,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Split *text* into chunks whose approximate token length ≤ *max_tokens*.\n",
    "    \"\"\"\n",
    "    enc = _get_encoder(model)\n",
    "    try:\n",
    "        toks = enc.encode(text)\n",
    "        chunk_lists = [toks[i:i+max_tokens] for i in range(0, len(toks), max_tokens)]\n",
    "        if hasattr(enc, \"decode\"):\n",
    "            return [enc.decode(chunk) for chunk in chunk_lists]\n",
    "        raise AttributeError\n",
    "    except AttributeError:\n",
    "        char_step = max_tokens * 4\n",
    "        return [text[i:i+char_step] for i in range(0, len(text), char_step)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3116f8-c5d2-44f3-a378-a18d22f77e04",
   "metadata": {},
   "source": [
    "## Merge LLM Output and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbddb0c3-60a0-4c13-ad24-02f174efcafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dedup_list(lst: List[Any]) -> List[Any]:\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for item in lst:\n",
    "        key = json.dumps(item, sort_keys=True, default=str)\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            out.append(item)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db286de9-796f-49b1-a218-133e2b90a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dedup_parties(parties: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for party in parties:\n",
    "        key = party.get(\"name\", \"\").strip().lower()\n",
    "        if key and key not in seen:\n",
    "            seen.add(key)\n",
    "            out.append(party)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f37a530-e8ac-49cc-ba8c-4de62484c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dedup_clauses(clauses: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for clause in clauses:\n",
    "        key = (clause.get(\"name\", \"\").strip().lower(), bool(clause.get(\"present\")))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            out.append(clause)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95ca0e6b-6c68-44ff-9263-b4a40ba79233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _postprocess_contract(data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Remove duplicates inside a single contract JSON.\"\"\"\n",
    "    if not isinstance(data, dict):\n",
    "        return data\n",
    "    if \"parties\" in data and isinstance(data[\"parties\"], list):\n",
    "        data[\"parties\"] = _dedup_parties(data[\"parties\"])\n",
    "    if \"clauses\" in data and isinstance(data[\"clauses\"], list):\n",
    "        data[\"clauses\"] = _dedup_clauses(data[\"clauses\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82bb9181-8ba4-40de-bbb6-adf3a759691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_json_objects(objs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    if not objs:\n",
    "        return {}\n",
    "    result: Dict[str, Any] = {}\n",
    "    for obj in objs:\n",
    "        for k, v in obj.items():\n",
    "            if v in (None, [], {}, \"\", 0):\n",
    "                continue\n",
    "            if k not in result or result[k] in (None, [], {}, \"\", 0):\n",
    "                result[k] = v\n",
    "            else:\n",
    "                if isinstance(v, list) and isinstance(result[k], list):\n",
    "                    result[k] = _dedup_list(result[k] + v)\n",
    "                elif isinstance(v, dict) and isinstance(result[k], dict):\n",
    "                    merged = result[k]\n",
    "                    merged.update({kk: vv for kk, vv in v.items() if vv not in (None, [], {}, \"\", 0)})\n",
    "                    result[k] = merged\n",
    "    return _postprocess_contract(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740fc4c2-1822-4043-8950-2eee86cc5ce7",
   "metadata": {},
   "source": [
    "## Clear json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33cf6648-fd03-423c-8829-a448f35e9d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_JSON_FENCE_RE = re.compile(r\"```[a-zA-Z0-9_]*|```\")\n",
    "\n",
    "def _strip_markdown_fence(text:str)->str:\n",
    "    return _JSON_FENCE_RE.sub(\"\", text).strip()\n",
    "\n",
    "def _remove_trailing_commas(s:str)->str:\n",
    "    # commas before } or ]\n",
    "    return re.sub(r\",\\s*([}\\]])\", r\"\\\\1\", s)\n",
    "\n",
    "def _add_missing_commas(s:str)->str:\n",
    "    # \"next_key\" -> \"value\"\n",
    "    return re.sub(r'(\"\\s*)(\"[a-zA-Z0-9_]+\"\\s*:)', r'\", \\2', s)\n",
    "\n",
    "CLEAN_STEPS = (_strip_markdown_fence, _remove_trailing_commas, _add_missing_commas)\n",
    "\n",
    "def fix_and_load(blob:str)->dict|None:\n",
    "    \"\"\"Try json.loads; if it fails run and retry.\"\"\"\n",
    "    try:\n",
    "        return json.loads(blob)\n",
    "    except Exception as e:\n",
    "        cleaned = blob\n",
    "        for fn in CLEAN_STEPS:\n",
    "            cleaned = fn(cleaned)\n",
    "        try:\n",
    "            return json.loads(cleaned)\n",
    "        except Exception as e2:\n",
    "            log.debug(\"JSON repair failed: %s\", e2)\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82b930d-88f8-415c-b8a3-3aaeab729753",
   "metadata": {},
   "source": [
    "# SCHEMA MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1936474-7fb8-4830-974f-c22031f54d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_MAP: Dict[str, Type[GenericContractSchema]] = {\n",
    "    EmploymentSchema().contract_type.lower(): EmploymentSchema,\n",
    "    NDASchema().contract_type.lower(): NDASchema,\n",
    "    MSASchema().contract_type.lower(): MSASchema,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e72002e3-2eec-4f18-8cbb-cdb950457b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _schema_menu_json() -> str:\n",
    "    menu = {}\n",
    "    for name, schema_cls in SCHEMA_MAP.items():\n",
    "        menu[name] = schema_cls.schema()\n",
    "    menu[\"generic\"] = GenericContractSchema.schema()\n",
    "    return json.dumps(menu, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2916404-bcd5-4863-be54-1277e7893fd6",
   "metadata": {},
   "source": [
    "# Metadata Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99ea358-0455-4bf0-962c-25164f8c2a7c",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc9ddf31-ef17-447a-8c30-d60514556cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = dedent(\"\"\"You are an expert contract analyst.\n",
    "\n",
    "**Goal**  \n",
    "Return a single **valid JSON object** that follows the selected schema and\n",
    "contains the best structured metadata you can extract.\n",
    "\n",
    "### 1. Decide `contract_type`\n",
    "Give the document a short label (one or two words).\n",
    "\n",
    "### 2. Pick schema\n",
    "Select the schema from the menu whose name is closest to that label.\n",
    "If nothing fits, pick `generic`.\n",
    "\n",
    "### 3. Extract\n",
    "Fill every key. Use null where data is absent.  \n",
    "Unknown but important extra fields → `custom_fields` (key/value).  \n",
    "Detect common clauses (indemnification, confidentiality, non‑compete, etc.)\n",
    "and list them in `clauses`.\n",
    "\n",
    "### Output rules\n",
    "- **Return only JSON** (no markdown, no explanations).  \n",
    "- Must parse with `json.loads` on first try.\n",
    "\n",
    "### Schema menu\n",
    "{schema_menu}\n",
    "\n",
    "### Contract\n",
    "{contract_text}\n",
    "### End\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc4a0607-0fcc-498c-8a37-0d650b8e9a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_json(blob: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Extract the *largest* JSON object embedded in `blob`.\n",
    "\n",
    "    Using the first \"{\" and the last \"}\" is a robust way to keep the whole\n",
    "    object even when the model spills log‑probs or other text before / after.\n",
    "    \"\"\"\n",
    "    start = blob.find(\"{\")\n",
    "    end = blob.rfind(\"}\")\n",
    "    if start == -1 or end == -1 or end <= start:\n",
    "        return None\n",
    "    return blob[start : end + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebe2f9d9-4f1e-4ea1-9491-1025a6a68090",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _extract_single(\n",
    "    pdf: Path, text: str, client: LLMClient\n",
    ") -> GenericContractSchema:\n",
    "    \"\"\"\n",
    "    Call the LLM once for a text *chunk* and return a parsed schema instance.\n",
    "\n",
    "    Includes automatic JSON‑repair fallback.\n",
    "    \"\"\"\n",
    "    prompt = PROMPT.format(schema_menu=_schema_menu_json(), contract_text=text)\n",
    "    messages = [ChatMessage(role=\"system\", content=prompt)]\n",
    "\n",
    "    start_ts = time.perf_counter()\n",
    "    raw = await client.chat_completion(messages)\n",
    "    latency = time.perf_counter() - start_ts\n",
    "\n",
    "    json_blob = _extract_json(raw) or raw\n",
    "    data = fix_and_load(json_blob)\n",
    "    if data is None:\n",
    "        log.error(\n",
    "            \"Unrecoverable JSON for %s (first 120 chars): %s\",\n",
    "            pdf.name,\n",
    "            json_blob[:120],\n",
    "        )\n",
    "        return GenericContractSchema(contract_type=\"unknown\", parties=[])\n",
    "\n",
    "    schema_cls: Type[GenericContractSchema] = SCHEMA_MAP.get(\n",
    "        data.get(\"contract_type\", \"\").lower(), GenericContractSchema\n",
    "    )\n",
    "    extraction = schema_cls(**data)\n",
    "    log.info(\"Extracted chunk for %s in %.1fs\", pdf.name, latency)\n",
    "    return extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d82caf48-f2fb-42ee-ac6d-361669f2be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_metadata(\n",
    "    pdf: Path,\n",
    "    client: LLMClient,\n",
    "    *,\n",
    "    save: bool = True,\n",
    "    output_dir: Optional[Path] = None,\n",
    "    max_tokens: int = MAX_TOKENS_DEFAULT,\n",
    ") -> GenericContractSchema:\n",
    "    \"\"\"\n",
    "    High‑level orchestration for a *single PDF*.\n",
    "\n",
    "    1. Load & (if necessary) chunk the PDF text.\n",
    "    2. Run `_extract_single` on each chunk (concurrently limited by caller).\n",
    "    3. Merge the partial JSONs → deduplicate lists.\n",
    "    4. Persist result to disk (optional).\n",
    "    \"\"\"\n",
    "    text = load_pdf_text(pdf)\n",
    "    model_name = client.model\n",
    "\n",
    "    if token_len(text, model_name) <= max_tokens:\n",
    "        final_extraction = await _extract_single(pdf, text, client)\n",
    "    else:\n",
    "        chunks = split_by_tokens(text, max_tokens=max_tokens, model=model_name)\n",
    "        extractions: List[GenericContractSchema] = []\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            log.info(\"Processing chunk %d/%d for %s\", idx + 1, len(chunks), pdf.name)\n",
    "            ext = await _extract_single(pdf, chunk, client)\n",
    "            extractions.append(ext)\n",
    "\n",
    "        merged_json = merge_json_objects(\n",
    "            [e.model_dump(mode=\"json\", exclude_none=True) for e in extractions]\n",
    "        )\n",
    "        schema_cls: Type[GenericContractSchema] = SCHEMA_MAP.get(\n",
    "            merged_json.get(\"contract_type\", \"\").lower(), GenericContractSchema\n",
    "        )\n",
    "        final_extraction = schema_cls(**merged_json)\n",
    "\n",
    "    if save:\n",
    "        import hashlib\n",
    "\n",
    "        if output_dir:\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            short_hash = hashlib.md5(str(pdf).encode()).hexdigest()[:6]\n",
    "            json_name = f\"{pdf.stem}_{short_hash}.json\"\n",
    "            out_path = output_dir / json_name\n",
    "        else:\n",
    "            out_path = pdf.with_suffix(\".json\")\n",
    "\n",
    "        out_path.write_text(\n",
    "            final_extraction.model_dump_json(indent=2, exclude_none=True),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "        log.info(\"Wrote %s\", out_path)\n",
    "\n",
    "    return final_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c7237-5272-4520-a745-83a55fa45944",
   "metadata": {},
   "source": [
    "# Run Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60c24426-8c6d-420b-8489-32f34454f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _proc(pdf:Path,client:LLMClient,*,save:bool,od:Optional[Path]):\n",
    "    meta = await extract_metadata(pdf,client,save=save,output_dir=od)\n",
    "    if not save:\n",
    "        print(json.dumps(meta.model_dump(mode='json',exclude_none=True),indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "25674f20-19fd-4f53-b5de-db147e610391",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main(dir_path:Path,concurrency:int,save:bool,output_dir:Optional[Path]):\n",
    "    dir_path = Path(dir_path)\n",
    "    pdfs = [f for f in dir_path.rglob('*') if (\n",
    "            f.is_file()\n",
    "            and f.suffix.lower() == '.pdf' \n",
    "            and f.stat().st_size > 0\n",
    "        )\n",
    "    ]\n",
    "    if not pdfs:\n",
    "        raise SystemExit('No PDFs found')\n",
    "    async with LLMClient() as client:\n",
    "        await gather_with_concurrency(concurrency,*(_proc(p,client,save=save,od=output_dir) for p in pdfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d71c9481-49fc-4131-b49c-62d4ae55f9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_n/rk12tlkj4_zd449jbv9n6p0m0000gn/T/ipykernel_42917/932756393.py:4: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  menu[name] = schema_cls.schema()\n",
      "/var/folders/_n/rk12tlkj4_zd449jbv9n6p0m0000gn/T/ipykernel_42917/932756393.py:5: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  menu[\"generic\"] = GenericContractSchema.schema()\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "try:\n",
    "    asyncio.run(main(\"../data/full_contract_pdf\",2,True,Path(\"output\")))\n",
    "except KeyboardInterrupt:\n",
    "    print('Cancelled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01238e41-b7a1-4b78-bc27-b4703e658165",
   "metadata": {},
   "source": [
    "## The output you can find inside the notebook/output folder "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
